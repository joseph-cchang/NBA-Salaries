lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length = 100))
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=lambda.list.lasso, nfolds = 5)
plot(lasso.mod, xvar="lambda", label=TRUE)
cv.out.lasso = cv.glmnet(x.train,y.train,alpha=1)
plot(cv.out.lasso)
abline(v=log(cv.out.lasso$lambda.min), col="red", lwd=3, lty=2)
bestlam2 = cv.out.lasso$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam2, newx = x.test)
mean((lasso.pred-y.test)^2)
out = glmnet(x, y, alpha=1, lambda=lambda.list.lasso)
lasso.coef = predict(out, type="coefficients", s=bestlam)
lasso.coef
tree_our_data=tree(Salary~. , data = our_data)
summary(tree_our_data)
# plot the fitted tree
plot(tree_our_data)
text(tree_our_data, pretty = 0, cex = 1, col = "blue")
title("Decision tree on our_data", cex = 1)
draw.tree(tree_our_data, nodeinfo=TRUE, cex = 1)
# Fit model on training set
tree.nba = tree(Salary~. , data = training)
# Plot the tree
draw.tree(tree.nba, nodeinfo=TRUE, cex = 1)
title("Regression Tree Built on Training Set")
summary(tree.nba)
# Predict on test set
tree.pred = predict(tree.nba, testing, type="vector")
# mean squared error
mean((tree.pred-testing$Salary)^2)
# root mean squared error
sqrt(y)
set.seed(3152022)
# K=10-Fold cross validation
cv = cv.tree(tree.nba, K=10)
# the tree with smaller size
best.cv = min(cv$size[cv$dev == min(cv$dev)])
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev, type="b", xlab = "Number of leaves, \'best\'",
ylab = "CV Misclassification Error", col = "red", main="CV")
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv$dev) # Get minimum error index
abline(h = cv$dev[min.error],lty = 2)
# Prune tree
pt.cv = prune.tree(tree.nba, best=best.cv)
# # Plot pruned tree
plot(pt.cv)
text(pt.cv, pretty=0, col = "blue", cex = .5)
title("Pruned tree of size")
# Predict on test set
pred.pt.cv = predict(pt.cv, testing, type="vector")
# mean squared error
mean((pred.pt.cv-testing$Salary)^2)
# root mean squared error
sqrt(w)
# Predict on test set
pred.pt.cv = predict(pt.cv, testing, type="vector")
# mean squared error
mean((pred.pt.cv-testing$Salary)^2)
# root mean squared error
sqrt(mean((pred.pt.cv-testing$Salary)^2))
# Predict on test set
tree.pred = predict(tree.nba, testing, type="vector")
# mean squared error
y = mean((tree.pred-testing$Salary)^2)
# root mean squared error
sqrt(y)
# mean squared error
mean((pred.pt.cv-testing$Salary)^2)
# root mean squared error
sqrt(mean((pred.pt.cv-testing$Salary)^2))
# plot the fitted tree
plot(tree_our_data)
text(tree_our_data, pretty = 0, cex = 0.6, col = "blue")
title("Decision tree on our_data", cex = 1)
draw.tree(tree_our_data, nodeinfo=TRUE, cex = 0.6)
# Fit model on training set
tree.nba = tree(Salary~. , data = training)
# Plot the tree
draw.tree(tree.nba, nodeinfo=TRUE, cex = 0.6)
title("Regression Tree Built on Training Set")
varImpPlot(rf_our_data, sort=T, main="Variable Importance for rf_our_data", n.var=5)
varImpPlot(rf_our_data, sort=T, main="Variable Importance for rf_our_data", n.var=5, cex = 0.8)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
library(ggplot2)
library(GGally)
library(tidyverse)
library(glmnet)
library(knitr)
library(dplyr)
library(tidyverse)
library(modelr)
library(pander)
library(corrplot)
library(readxl)
library(ISLR)
library(tidymodels)
library(ggthemes)
library(naniar)
library(ROCR)
library(maptree)
library(tree)
library(factoextra)
library(cluster)
library(randomForest)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
library(ggplot2)
library(GGally)
library(tidyverse)
library(glmnet)
library(knitr)
library(dplyr)
library(tidyverse)
library(modelr)
library(pander)
library(corrplot)
library(readxl)
library(ISLR)
library(tidymodels)
library(ggthemes)
library(naniar)
library(ROCR)
library(maptree)
library(tree)
library(factoextra)
library(cluster)
library(randomForest)
# NBA stuffer
X2020_2021_NBA_Stats_Player_Box_Score_Advanced_Metrics <- read_excel("2020-2021 NBA Stats  Player Box Score  Advanced Metrics.xlsx")
bball_stats <- as.data.frame(X2020_2021_NBA_Stats_Player_Box_Score_Advanced_Metrics)
my_colnames <- c('Rank', 'Player', 'Team', 'Position', 'Age', 'Games_Played', 'MPG', 'Minutes_percent', 'Usage_Rate' , 'Turnover_rate', 'free_throws_attempted', 'Free_throw_percent', '2-point_field goals_attempted', '2-point_percent', '3-point_field_goals_attempted', 'three_point_percent', 'effective_shooting_percent' , 'True_shooting_percent', 'PPG', 'RPG', 'Total_rebound_percent', 'APG', 'Assist_percent', 'SPG' ,'BPG', 'TPG', 'Versatility_Index', 'Offensive_Rating' , 'Defensive_Rating')
colnames(bball_stats) <- my_colnames
new_bball_stats <- bball_stats[-1,-1]
# Kaggle
labels <- c('Player','Position', 'Age', 'Team', 'Games', 'Minutes_played', 'Player_Efficiency_Rating', 'true_shooting_percent', '3-point_attempt_rate', 'free-throw_attempt_rate', 'offensive_rebound_ percentage', 'defensive_rebound_percentage', 'total_rebound_percentage', 'assist_percentage', 'steal_percentage', 'block_percentage', 'turnover_percentage', 'usage_rate', 'offensive_win_shares', 'defensive_win_shares', 'win_shares','win_shares_per_48_minutes', 'Offensive_Box_Plus_Minus', 'Defensive_Box_Plus_Minus','Box_Plus_Minus','Value_Over_Replacement')
data_advanced <- read.csv("nba2021_advanced.csv", col.names = labels, na= "XXX")
# Basektball-reference salaries
labels2 <- c('Rank', 'Player', 'Salary', 'Use', 'Guaranteed')
salaries <- read.csv("nba_salaries_21-22.csv", col.names = labels2)
combine <- inner_join(new_bball_stats, data_advanced, by = 'Player') %>%
inner_join(salaries, by = 'Player')
selection <- subset(combine, select= -c(2:4,29:33,35:45,54,56:57))
less_data <- selection %>% relocate(c(PPG,RPG,APG,SPG,BPG,TPG), .before = MPG)
conversion <- less_data[,2:35] %>% mutate_if(is.character,as.numeric) %>% mutate(Total_Minutes = Games_Played*MPG)
conversion1 <- conversion %>% add_column(less_data$Player)
names(conversion1)[names(conversion1) == "less_data$Player"] <- "Player"
conversion2 <- conversion1 %>% relocate((Player), .before = Games_Played)
conversion3 <- conversion2 %>% relocate((Total_Minutes), .before=MPG)
conversion3
reduced_data <- conversion3 %>% filter(Games_Played >= 7 & Total_Minutes >= 336) %>% select(Player:Salary) %>% drop_na()
reduced_data
conversion3
reduced_data <- conversion3 %>% filter(Games_Played >= 7 & Total_Minutes >= 336) %>% select(Player:Salary) %>% drop_na()
par(cex=1)
my_cor <- reduced_data %>% select_if(is.numeric) %>% drop_na() %>% cor() %>% round(3)
corrplot(my_cor, method = "circle", type = "upper", cex.pch=10, tl.cex = 1)
salarycor <- reduced_data %>% select(Salary, Games_Played:Value_Over_Replacement) %>% drop_na()
corr_Salary <- cor(salarycor)[,"Salary"]
corr_Salary[which(corr_Salary > 0.6)]
less_data
our_data <- reduced_data %>% select(Player, Salary, PPG, APG, MPG, TPG, MPG, Minutes_percent, free_throws_attempted, Offensive_Box_Plus_Minus, Value_Over_Replacement)
hist(our_data$Salary, main = "Histogram of NBA salaries 2021-2022", xlab="Salary Amount", breaks = "Sturges", labels = TRUE)
op=par(mfrow=c(2,2))
plot(our_data$Salary, our_data$PPG, main="Scatterplot of Salary vs. PPG",
xlab="Salary", ylab="PPG", pch=19)
abline(lm(our_data$PPG~our_data$Salary), col="red")
plot(our_data$Salary, our_data$APG, main="Scatterplot of Salary vs. APG",
xlab="Salary", ylab="APG", pch=19)
abline(lm(our_data$APG~our_data$Salary), col="red")
plot(our_data$Salary, our_data$MPG, main="Scatterplot of Salary vs. MPG",
xlab="Salary", ylab="MPG", pch=19)
abline(lm(our_data$MPG~our_data$Salary), col="red")
plot(our_data$Salary, our_data$TPG, main="Scatterplot of Salary vs. TPG",
xlab="Salary", ylab="TPG", pch=19)
abline(lm(our_data$TPG~our_data$Salary), col="red")
plot(our_data$Salary, our_data$Minutes_percent, main="Scatterplot of Salary vs. Minutes Percent", xlab="Salary", ylab="Minutes Percent", pch=19)
abline(lm(our_data$Minutes_percent~our_data$Salary), col="red")
plot(our_data$Salary, our_data$free_throws_attempted, main="Scatterplot of Salary vs. Free Throws Attempted", xlab="Salary", ylab="Free Throws Attempted", pch=19)
abline(lm(our_data$free_throws_attempted~our_data$Salary), col="red")
plot(our_data$Salary, our_data$Offensive_Box_Plus_Minus, main="Scatterplot of Salary vs. Offensive Box Plus Minus", xlab="Salary", ylab="Offensive Box Plus Minus", pch=19)
abline(lm(our_data$Offensive_Box_Plus_Minus~our_data$Salary), col="red")
plot(our_data$Salary, our_data$Value_Over_Replacement, main="Scatterplot of Salary vs. Value Over Replacement", xlab="Salary", ylab="Value Over Replacement", pch=19)
abline(lm(our_data$Value_Over_Replacement~our_data$Salary), col="red")
set.seed(3152022)
fit_data <- our_data[-1]
new_data <- resample_partition(fit_data, p = c(test=0.2, train=0.8))
training <- as.data.frame(new_data$train)
testing <- as.data.frame(new_data$test)
training
testing
fit_data
386*0.8
fit1 <- lm(Salary ~ PPG + APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training)
summary(fit1)
train.predict1 <- predict(fit1, training)
test.predict1 <- predict(fit1, testing)
mean((train.predict1-training$Salary)^2)
mean((test.predict1-testing$Salary)^2)
fit2 <- lm(Salary ~ PPG + APG + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data= training)
summary(fit2)
fit2 <- lm(Salary ~ PPG + APG + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data= training)
summary(fit2)
train.predict2 <- predict(fit2, training)
test.predict2 <- predict(fit2, testing)
mean((train.predict2-training$Salary)^2)
mean((test.predict2-testing$Salary)^2)
train.predict2 <- predict(fit2, training)
test.predict2 <- predict(fit2, testing)
mean((train.predict2-training$Salary)^2)
mean((test.predict2-testing$Salary)^2)
fit3 <- lm(Salary ~ PPG + APG + Value_Over_Replacement, data= training)
summary(fit3)
fit3 <- lm(Salary ~ PPG + APG, data= training)
summary(fit3)
fit1 <- lm(Salary ~ PPG + APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training)
summary(fit1)
fit2 <- lm(Salary ~ APG + Value_Over_Replacement, data= training)
summary(fit2)
fit3 <- lm(Salary ~ PPG, data= training)
summary(fit3)
par(cex=1)
my_cor <- reduced_data %>% select_if(is.numeric) %>% drop_na() %>% cor() %>% round(3)
corrplot(my_cor, method = "circle", type = "upper", cex.pch=10, tl.cex = 1)
salarycor <- reduced_data %>% select(Salary, Games_Played:Value_Over_Replacement) %>% drop_na()
corr_Salary <- cor(salarycor)[,"Salary"]
corr_Salary[which(corr_Salary > 0.6)]
fit3 <- lm(Salary ~ PPG + MPG, data= training)
summary(fit3)
train.predict3 <- predict(fit3, training)
test.predict3 <- predict(fit3, testing)
mean((train.predict3-training$Salary)^2)
mean((test.predict3-testing$Salary)^2)
mean_train_salary <- mean(training$Salary)
mean_train_salary
mean_train_salary <- mean(training$Salary)
training_salarymean <- training %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
training_salarymean
training_salarymean <- training[-1] %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
mean_train_salary <- mean(training$Salary)
training_salarymean <- training[-1,] %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
training_salarymean
training_salarymean <- training[,-1] %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
training_salarymean <- training[] %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
training_salarymean
training_logit <- glm(salarygreater ~ PPG + APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training_salarymean, family = 'binomial')
salarymean_pred_training <- predict(training_logit, training_salarymean, type="response")
train_maj_rule <- ifelse(salarymean_pred_training > 0.5, 1,0)
salarymean_pred_training
calc_error_rate <- function(predicted.value, true.value){
return(mean(true.value!=predicted.value))}
calc_error_rate(train_maj_rule, training_salarymean$salarygreater)
testing_salarymean<- testing %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
salarymean_pred_testing <- predict(training_logit,  testing_salarymean, type="response")
test_maj_rule2 <- ifelse(salarymean_pred_testing > 0.5, 1,0)
calc_error_rate(test_maj_rule2, testing_salarymean$salarygreater)
testing_salarymean
pred <- prediction(salarymean_pred_testing, testing_salarymean$salarygreater)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, col = 2, lwd = 3, main = "ROC curve")
abline(0, 1)
auc <- performance(pred, "auc")@y.values[[1]]
auc
x <- model.matrix(Salary ~ ., data= fit_data)
y <- fit_data$Salary
x.train <- as.matrix(training[,-1])
y.train <- as.matrix(training$Salary)
x.test <- as.matrix(testing[,-1])
y.test <- as.matrix(testing$Salary)
x <- model.matrix(Salary ~ ., data= our_data)
x
x <- model.matrix(Salary ~ ., data= fit_data)
x
x.train
x <- model.matrix(Salary ~ ., data= fit_data)
y <- fit_data$Salary
x.train <- as.matrix(training[,-1])
y.train <- as.matrix(training$Salary)
x.test <- as.matrix(testing[,-1])
y.test <- as.matrix(testing$Salary)
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))
ridge.mod = cv.glmnet(x.train, y.train, alpha=0,lambda=lambda.list.ridge, nfolds=5)
ridge.pred_1 = predict(ridge.mod, s = ridge.mod$lambda.min, type="coefficients", newx=x.test)
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))
ridge.mod = cv.glmnet(x.train, y.train, alpha=0,lambda=lambda.list.ridge, nfolds=5)
ridge.pred_1 = predict(ridge.mod, s = ridge.mod$lambda.min, type="coefficients", newx=x.test)
ridge.pred=predict(ridge.mod, s = ridge.mod$lambda.min, newx=x.test)
mean((ridge.pred-y.test)^2)
set.seed(3152022)
cv.out.ridge = cv.glmnet(x.train, y.train, alpha= 0)
plot(cv.out.ridge)
abline(v=log(cv.out.ridge$lambda.min), col = "blue", lwd=3, lty=2)
bestlam = cv.out.ridge$lambda.min
ridge.pred=predict(ridge.mod, s = bestlam, newx=x.test)
mean((ridge.pred-y.test)^2)
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))
ridge.mod = cv.glmnet(x.train, y.train, alpha=0,lambda=lambda.list.ridge, nfolds=5)
ridge.pred_1 = predict(ridge.mod, s = ridge.mod$lambda.min, type="coefficients", newx=x.test)
ridge.pred=predict(ridge.mod, s = ridge.mod$lambda.min, newx=x.test)
mean((ridge.pred-y.test)^2)
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))
ridge.mod = cv.glmnet(x.train, y.train, alpha=0,lambda=lambda.list.ridge, nfolds=5)
ridge.pred_1 = predict(ridge.mod, s = ridge.mod$lambda.min, type="coefficients", newx=x.test)
ridge.pred=predict(ridge.mod, s = ridge.mod$lambda.min, newx=x.test)
mean((ridge.pred-y.test)^2)
out = glmnet(x,y,alpha=0)
predict(out, type="coefficients", s=bestlam)
set.seed(3152022)
lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length = 100))
lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=lambda.list.lasso, nfolds = 5)
plot(lasso.mod, xvar="lambda", label=TRUE)
cv.out.lasso = cv.glmnet(x.train,y.train,alpha=1)
plot(cv.out.lasso)
abline(v=log(cv.out.lasso$lambda.min), col="red", lwd=3, lty=2)
bestlam2 = cv.out.lasso$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam2, newx = x.test)
mean((lasso.pred-y.test)^2)
out = glmnet(x, y, alpha=1, lambda=lambda.list.lasso)
lasso.coef = predict(out, type="coefficients", s=bestlam)
lasso.coef
tree_our_data=tree(Salary~. , data = our_data)
summary(tree_our_data)
tree_our_data=tree(Salary~. , data = fit_data)
summary(tree_our_data)
tree_our_data=tree(Salary~. , data = our_data)
summary(tree_our_data)
tree_our_data=tree(Salary~. , data = our_data)
summary(tree_our_data)
plot(tree_our_data)
text(tree_our_data, pretty = 0, cex = 0.4, col = "blue")
title("Decision tree on our_data", cex = 1)
draw.tree(tree_our_data, nodeinfo=TRUE, cex = 0.4)
plot(tree_our_data)
text(tree_our_data, pretty = 0, cex = 0.4, col = "blue")
title("Decision tree on whole data", cex = 1)
draw.tree(tree_our_data, nodeinfo=TRUE, cex = 0.4)
# Fit model on training set
tree.nba = tree(Salary~. , data = training)
draw.tree(tree.nba, nodeinfo=TRUE, cex = 0.6)
title("Regression Tree Built on Training Set")
summary(tree.nba)
# Predict on test set
tree.pred = predict(tree.nba, testing, type="vector")
# mean squared error
y = mean((tree.pred-testing$Salary)^2)
# root mean squared error
sqrt(y)
# Predict on test set
tree.pred = predict(tree.nba, testing, type="vector")
y = mean((tree.pred-testing$Salary)^2)
sqrt(y)
set.seed(3152022)
# K=10-Fold cross validation
cv = cv.tree(tree.nba, K=10)
# the tree with smaller size
best.cv = min(cv$size[cv$dev == min(cv$dev)])
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev, type="b", xlab = "Number of leaves, \'best\'",
ylab = "CV Misclassification Error", col = "red", main="CV")
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv$dev) # Get minimum error index
abline(h = cv$dev[min.error],lty = 2)
y
set.seed(3152022)
# K=10-Fold cross validation
cv = cv.tree(tree.nba, K=10)
# the tree with smaller size
best.cv = min(cv$size[cv$dev == min(cv$dev)])
# Plot size vs. cross-validation error rate
plot(cv$size , cv$dev, type="b", xlab = "Number of leaves, \'best\'",
ylab = "CV Misclassification Error", col = "red", main="CV")
abline(v=best.cv, lty=2)
# Add lines to identify complexity parameter
min.error = which.min(cv$dev) # Get minimum error index
abline(h = cv$dev[min.error],lty = 2)
# Prune tree
pt.cv = prune.tree(tree.nba, best=best.cv)
# # Plot pruned tree
plot(pt.cv)
text(pt.cv, pretty=0, col = "blue", cex = .5)
title("Pruned tree of size")
# Predict on test set
pred.pt.cv = predict(pt.cv, testing, type="vector")
# mean squared error
mean((pred.pt.cv-testing$Salary)^2)
# root mean squared error
sqrt(mean((pred.pt.cv-testing$Salary)^2))
rf_our_data = randomForest(salarygreater ~ PPG + APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training_salarymean, mtry=3, ntree=500, importance=TRUE)
rf_our_data = randomForest(salarygreater ~ PPG + APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training_salarymean, mtry=3, ntree=500, importance=TRUE)
rf_our_data
yhat.rf = predict (rf_our_data, newdata = testing_salarymean)
# Confusion matrix
rf.err = table(pred = yhat.rf, truth = testing_salarymean$salarygreater)
test.rf.err = 1 - sum(diag(rf.err))/sum(rf.err)
test.rf.err
varImpPlot(rf_our_data, sort=T, main="Variable Importance for rf_our_data", n.var=5, cex = 0.8)
salary_prediction <- function(m, points, assists, minutes, turnovers, minutes_p, free_throws, offense_plus_minus, value){
pre_new <- predict(m, data.frame(PPG = points, APG = assists, MPG= minutes, TPG =  turnovers, Minutes_percent = minutes_p, free_throws_attempted = free_throws, Offensive_Box_Plus_Minus = offense_plus_minus, Value_Over_Replacement = value))
msg <- paste("PPG:", points, ",APG:", assists, ",MPG:", minutes, ",TPG:", turnovers, ",Minutes_percent:", minutes_p, ",free_throws_attempted:", free_throws, ",Offensive_Box_Plus_Minus:", offense_plus_minus, ",Value_Over_Replacement", value ," ==> Expected Salary: $", format(round(pre_new), big.mark = ","), sep = "")
print(msg)
}
model <- lm(Salary ~ PPG+ APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training)
our_data[which(our_data$Player == "Danny Green"),]
salary_prediction(m = model, points = 9.5, assists = 1.7, minutes = 28, turnovers = 0.96, minutes_p = 58.4, free_throws = 40, offense_plus_minus = -1.1, value = 0.2)
salary_prediction <- function(m, points, assists, minutes, turnovers, minutes_p, free_throws, offense_plus_minus, value){
pre_new <- predict(m, data.frame(PPG = points, APG = assists, MPG= minutes, TPG =  turnovers, Minutes_percent = minutes_p, free_throws_attempted = free_throws, Offensive_Box_Plus_Minus = offense_plus_minus, Value_Over_Replacement = value))
msg <- paste("PPG:", points, ", APG:", assists, ", MPG:", minutes, ", TPG:", turnovers, ", Minutes_percent:", minutes_p, ", free_throws_attempted:", free_throws, ", Offensive_Box_Plus_Minus:", offense_plus_minus, ", Value_Over_Replacement", value ," ==> Expected Salary: $", format(round(pre_new), big.mark = ","), sep = "")
print(msg)
}
model <- lm(Salary ~ PPG+ APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training)
our_data[which(our_data$Player == "Danny Green"),]
salary_prediction(m = model, points = 9.5, assists = 1.7, minutes = 28, turnovers = 0.96, minutes_p = 58.4, free_throws = 40, offense_plus_minus = -1.1, value = 0.2)
salary_prediction <- function(m, points, assists, minutes, turnovers, minutes_p, free_throws, offense_plus_minus, value){
pre_new <- predict(m, data.frame(PPG = points, APG = assists, MPG= minutes, TPG =  turnovers, Minutes_percent = minutes_p, free_throws_attempted = free_throws, Offensive_Box_Plus_Minus = offense_plus_minus, Value_Over_Replacement = value))
msg <- paste("PPG:", points, ", APG:", assists, ", MPG:", minutes, ", TPG:", turnovers, ", Minutes_percent:", minutes_p, ", free_throws_attempted:", free_throws, ", Offensive_Box_Plus_Minus:", offense_plus_minus, ", Value_Over_Replacement:", value ," ==> Expected Salary: $", format(round(pre_new), big.mark = ","), sep = "")
print(msg)
}
model <- lm(Salary ~ PPG+ APG + MPG + TPG + Minutes_percent + free_throws_attempted + Offensive_Box_Plus_Minus + Value_Over_Replacement, data = training)
our_data[which(our_data$Player == "Danny Green"),]
salary_prediction(m = model, points = 9.5, assists = 1.7, minutes = 28, turnovers = 0.96, minutes_p = 58.4, free_throws = 40, offense_plus_minus = -1.1, value = 0.2)
our_data[which(our_data$Player == "Bradley Beal"),]
salary_prediction(m = model, points = 31.3, assists = 4.4, minutes = 35.8, turnovers = 3.12, minutes_p = 74.5, free_throws = 459, offense_plus_minus = 6.3, value = 1.5)
our_data[which(our_data$Player == "Jayson Tatum"),]
salary_prediction(m = model, points = 26.4, assists = 4.3, minutes = 35.8, turnovers = 2.67, minutes_p = 74.5, free_throws = 340, offense_plus_minus = 4.6, value = 1.4)
summary(fit1)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
stats_salary_cor <-
our_data %>%
select(PPG, APG, MPG, TPG, MPG, Minutes_percent, free_throws_attempted, Offensive_Box_Plus_Minus, Value_Over_Replacement)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
library(ggplot2)
library(GGally)
library(tidyverse)
library(glmnet)
library(knitr)
library(dplyr)
library(tidyverse)
library(modelr)
library(pander)
library(corrplot)
library(readxl)
library(ISLR)
library(tidymodels)
library(ggthemes)
library(naniar)
library(ROCR)
library(maptree)
library(tree)
library(factoextra)
library(cluster)
library(randomForest)
# NBA stuffer
X2020_2021_NBA_Stats_Player_Box_Score_Advanced_Metrics <- read_excel("2020-2021 NBA Stats  Player Box Score  Advanced Metrics.xlsx")
bball_stats <- as.data.frame(X2020_2021_NBA_Stats_Player_Box_Score_Advanced_Metrics)
my_colnames <- c('Rank', 'Player', 'Team', 'Position', 'Age', 'Games_Played', 'MPG', 'Minutes_percent', 'Usage_Rate' , 'Turnover_rate', 'free_throws_attempted', 'Free_throw_percent', '2-point_field goals_attempted', '2-point_percent', '3-point_field_goals_attempted', 'three_point_percent', 'effective_shooting_percent' , 'True_shooting_percent', 'PPG', 'RPG', 'Total_rebound_percent', 'APG', 'Assist_percent', 'SPG' ,'BPG', 'TPG', 'Versatility_Index', 'Offensive_Rating' , 'Defensive_Rating')
colnames(bball_stats) <- my_colnames
new_bball_stats <- bball_stats[-1,-1]
# Kaggle
labels <- c('Player','Position', 'Age', 'Team', 'Games', 'Minutes_played', 'Player_Efficiency_Rating', 'true_shooting_percent', '3-point_attempt_rate', 'free-throw_attempt_rate', 'offensive_rebound_ percentage', 'defensive_rebound_percentage', 'total_rebound_percentage', 'assist_percentage', 'steal_percentage', 'block_percentage', 'turnover_percentage', 'usage_rate', 'offensive_win_shares', 'defensive_win_shares', 'win_shares','win_shares_per_48_minutes', 'Offensive_Box_Plus_Minus', 'Defensive_Box_Plus_Minus','Box_Plus_Minus','Value_Over_Replacement')
data_advanced <- read.csv("nba2021_advanced.csv", col.names = labels, na= "XXX")
# Basektball-reference salaries
labels2 <- c('Rank', 'Player', 'Salary', 'Use', 'Guaranteed')
salaries <- read.csv("nba_salaries_21-22.csv", col.names = labels2)
combine <- inner_join(new_bball_stats, data_advanced, by = 'Player') %>%
inner_join(salaries, by = 'Player')
selection <- subset(combine, select= -c(2:4,29:33,35:45,54,56:57))
less_data <- selection %>% relocate(c(PPG,RPG,APG,SPG,BPG,TPG), .before = MPG)
conversion <- less_data[,2:35] %>% mutate_if(is.character,as.numeric) %>% mutate(Total_Minutes = Games_Played*MPG)
conversion1 <- conversion %>% add_column(less_data$Player)
names(conversion1)[names(conversion1) == "less_data$Player"] <- "Player"
conversion2 <- conversion1 %>% relocate((Player), .before = Games_Played)
conversion3 <- conversion2 %>% relocate((Total_Minutes), .before=MPG)
reduced_data <- conversion3 %>% filter(Games_Played >= 7 & Total_Minutes >= 336) %>% select(Player:Salary) %>% drop_na()
par(cex=1)
my_cor <- reduced_data %>% select_if(is.numeric) %>% drop_na() %>% cor() %>% round(3)
corrplot(my_cor, method = "circle", type = "upper", cex.pch=10, tl.cex = 1)
salarycor <- reduced_data %>% select(Salary, Games_Played:Value_Over_Replacement) %>% drop_na()
corr_Salary <- cor(salarycor)[,"Salary"]
corr_Salary[which(corr_Salary > 0.6)]
stats_salary_cor <-
our_data %>%
select(PPG, APG, MPG, TPG, MPG, Minutes_percent, free_throws_attempted, Offensive_Box_Plus_Minus, Value_Over_Replacement)
ggpairs(stats_salary_cor)
cor(stats_salary_cor)[,"our_data"]
stats_salary_cor <-
our_data %>%
select(PPG, APG, MPG, TPG, MPG, Minutes_percent, free_throws_attempted, Offensive_Box_Plus_Minus, Value_Over_Replacement)
ggpairs(stats_salary_cor)
ggpairs
ggpairs(stats_salary_cor)
plot(our_data$Salary, our_data$Value_Over_Replacement, main="Scatterplot of Salary vs. Value Over Replacement", xlab="Salary", ylab="Value Over Replacement", pch=19)
ggpairs(stats_salary_cor)
# knit options
knitr::opts_chunk$set(echo = F,
results = 'markup',
fig.width = 4,
fig.height = 3,
fig.align = 'center',
message = F,
warning = F)
# packages
library(tidyverse)
library(broom)
library(faraway)
