---
title: "PSTAT131 Final Project"
author: "Joseph Chang, Tom Wei, Akul Bajaj"
date: "3/11/2022"
output:
  html_document:
    code_folding: hide
df_print: paged
---

```{r setup, include=FALSE, echo = FALSE}
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
```

## packages

```{r}
#install.packages("tidyverse")
#install.packages("corrplot")
#install.packages("ggplot2")       
#install.packages("GGally")
#install.packages("factoextra")
#install.packages("cluster")
library(ggplot2)                     
library(GGally)
library(tidyverse)
library(glmnet)
library(knitr)
library(dplyr)
library(tidyverse)
library(modelr)
library(pander)
library(corrplot)
library(readxl)
library(ISLR)
library(tidymodels)
library(ggthemes)
library(naniar)
library(ROCR)
library(maptree)
library(tree)
library(factoextra)
library(cluster)
```

# Introduction

Our project is \_\_\_

## Read data

First, we read in the data that we downloaded. The first dataset is from a website called NBA stuffer, which contains many basketball statistics in an excel file. The second dataset is from kaggle and contains more basketball statistics, some different than the first dataset. The last dataset is from basketball-reference.com and lists out every players' salary in the 2021-2022 season. 

```{r}
# NBA stuffer
X2020_2021_NBA_Stats_Player_Box_Score_Advanced_Metrics <- read_excel("2020-2021 NBA Stats  Player Box Score  Advanced Metrics.xlsx")
bball_stats <- as.data.frame(X2020_2021_NBA_Stats_Player_Box_Score_Advanced_Metrics)
my_colnames <- c('Rank', 'Player', 'Team', 'Position', 'Age', 'Games.Played', 'MPG', 'Minutes%', 'Usage_Rate' , 'Turnover_rate', 'free_throws_attempted', 'Free_throw_percent', '2-point_field goals_attempted', '2-point%', '3-point_field_goals_attempted', 'three_point_percent', 'effective_shooting_%' , 'True_shooting_%', 'PPG', 'RPG', 'Total rebound %', 'APG', 'Assist %', 'SPG' ,'BPG', 'TPG', 'Versatility_Index', 'Offensive_Rating' , 'Defensive_Rating')
colnames(bball_stats) <- my_colnames
new_bball_stats <- bball_stats[-1,-1]

# kaggle
labels <- c('Player','Position', 'Age', 'Team', 'Games', 'Minutes_played', 'Player_Efficiency_Rating', 'true_shooting_%', '3-point attempt rate', 'free-throw attempt rate', 'offensive rebound percentage', 'defensive rebound percentage', 'total rebound percentage', 'assist percentage', 'steal percentage', 'block percentage', 'turnover percentage', 'usage_rate', 'offensive_win_shares', 'defensive_win_shares', 'win_shares','win shares per 48 minutes', 'Offensive_Box Plus/Minus', 'Defensive_Box Plus/Minus','Box_Plus_Minus','Value_Over_Replacement')
data_advanced <- read.csv("nba2021_advanced.csv", col.names = labels, na= "XXX")

# basektball-reference salaries
labels2 <- c('Rank', 'Player', 'Salary', 'Use', 'Guaranteed')
salaries <- read.csv("nba_salaries_21-22.csv", col.names = labels2)
```

## Merge data into "master" dataset

This is where we merge the three datasets into one "master" dataset. This "master" dataset removes duplicated columns as well as columns that show unrelated/insignificant statistics. In addition, this "master" dataset changes all columns to dbl for more conveinient use later on.

```{r}
combine <- inner_join(new_bball_stats, data_advanced, by = 'Player') %>% 
  inner_join(salaries, by = 'Player')
  
# cut down all predictors to only 24 predictors
selection <- subset(combine, select= -c(2:4,7,9,12:14,20,22,29:33,35:47,49:51,54,56:57))
selection <- subset(combine, select= -c(2:4,29:33,35:45,54,56:57))
less_data <- selection %>% relocate(c(PPG,RPG,APG,SPG,BPG,TPG), .before = MPG)

# Convert predictors in less_data all to dbl, not chr
conversion <- less_data[,2:22] %>% mutate_if(is.character,as.numeric)
conversion1 <- conversion %>% add_column(less_data$Player) 
names(conversion1)[names(conversion1) == "less_data$Player"] <- "Player"
conversion2 <- conversion1 %>% relocate((Player), .before = Games.Played)

```

offensive_win_shares, PER, free-throw attempted, min%

## filter data

We decided to filter out players who played more th

```{r}
# filter players with games played over 10, then filter with total minutes played over 100
reduced_data <- conversion2 %>% filter(Games.Played >= 10 & MPG > 10) %>% select(Player:Salary) %>% drop_na()
```

## Plot of response variable

```{r}
plot(reduced_data$Salary, ylab= "Salary", ylim=c(0, 50000000))
hist(reduced_data$Salary, main = "Histogram of NBA salaries 2021-2022", xlab="Salary Amount",breaks = "Sturges")
```

## Check to see Correlation

```{r}
# correlation to see predictors vs other predictors in our_data2
my_cor <- reduced_data %>% select_if(is.numeric) %>% drop_na() %>% cor() %>% round(3) 
corrplot(my_cor, method = "circle", type = "upper", cex.pch=10)

# second 
salarycor <- reduced_data %>% select(Salary,Games.Played,PPG,RPG,APG,SPG,BPG,TPG,MPG,Usage_Rate, Free_throw_percent, three_point_percent,Versatility_Index, Offensive_Rating, Defensive_Rating, win_shares, Box_Plus_Minus, Value_Over_Replacement) %>% drop_na()

cor(salarycor)[,"Salary"]

# Based from the correlation, we see that PPG > MPG > TPG > Value.Over.Replacement > APG > win.shares > Versatility Index > Box.Plus.Minus > Usage Rate > RPG > SPG > Free throw% > Offensive Rating > Defensive rating > BPG > Games.Played > three-point %

# We will filter those correlations above 0.5, meaning we will only use PPG, MPG, TPG,Value_Over_Replacement, APG, win_shares, Versatility_Index, Box_Plus_Minus, Usage_Rate.
```

```{r}
our_data <- reduced_data %>% select(Player, Salary, PPG, APG, MPG, TPG, Value_Over_Replacement, win_shares, Versatility_Index, Box_Plus_Minus, Usage_Rate)
```

## training and testing data

```{r}
# Set random seed
set.seed(3112022)

# Sample 80% observations as training data
fit_data <- our_data[-1] 
new_data <- resample_partition(fit_data, p = c(test=0.2, train=0.8)) 
training <- as.data.frame(new_data$train)
testing <- as.data.frame(new_data$test)
```

## Linear regression

```{r}
# linear regression for data

# training data
fit1 <- lm(Salary ~ PPG + MPG+TPG+Value_Over_Replacement+APG+win_shares+ Versatility_Index+ Box_Plus_Minus + Usage_Rate, data = training)
summary(fit1)

# predicted salaries for training and testing in fit1
# train.predict <- predict(fit, training)
# test.predict <- predict(fit, testing)
# 
# # MSE of training / testing
# mean((train.predict-training$Salary)^2)
# mean((test.predict-testing$Salary)^2)


# predictors that have high correlation with Salary (p-value less than 0.05)
fit2 <- lm(Salary ~ PPG + Usage_Rate + APG, data= training)
summary(fit2)

# predicted salaries for training and testing in fit1
train.predict2 <- predict(fit2, training)
test.predict2 <- predict(fit2, testing)

# MSE of training / testing
mean((train.predict2-training$Salary)^2)
mean((test.predict2-testing$Salary)^2)


# third and final fit model 
fit3 <- lm(Salary ~ PPG, data= fit_data)
summary(fit3)

# predicted salaries for training and testing in fit1
train.predict3 <- predict(fit3, training)
test.predict3 <- predict(fit3, testing)

# MSE of training / testing
mean((train.predict3-training$Salary)^2)
mean((test.predict3-testing$Salary)^2)

```

## regularized regression ??

## Logistic regression?

```{r}
# a new variable will be created called salarygreater. 
# This tests whether a player's salary is greater than the mean salary

mean_train_salary <- mean(training$Salary)
training_salarymean<- training %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
training_logit <- glm(salarygreater ~ PPG + MPG+TPG+Value_Over_Replacement+APG+win_shares+ Versatility_Index+ 
Box_Plus_Minus + Usage_Rate, data = training_salarymean, family = 'binomial')

# prediction
salarymean_pred_training <- predict(training_logit, training_salarymean, type="response")
train_maj_rule <- ifelse(salarymean_pred_training > 0.5, 1,0)

# testing error
calc_error_rate <- function(predicted.value, true.value){
  return(mean(true.value!=predicted.value))
}
calc_error_rate(train_maj_rule, training_salarymean$salarygreater)

#and do the same with the testing error:
testing_salarymean<- testing %>% mutate(salarygreater=factor(ifelse(Salary >= mean_train_salary, 1, 0), levels=c(0, 1)))
salarymean_pred_testing <- predict(training_logit,  testing_salarymean, type="response")
test_maj_rule2 <- ifelse(salarymean_pred_testing > 0.5, 1,0)
calc_error_rate(test_maj_rule2, testing_salarymean$salarygreater)

#Now we can plot an ROC curve for the test data:
pred <- prediction(salarymean_pred_testing, testing_salarymean$salarygreater)

perf <- performance(pred, measure = "tpr", x.measure = "fpr")

plot(perf, col = 2, lwd = 3, main = "ROC curve")

abline(0, 1)

#And calculate the area under the curve (AUC):
auc <- performance(pred, "auc")@y.values[[1]]
auc
```

## Training, Testing for Ridge/ Lasso

```{r}
x <- model.matrix(Salary~., fit_data)
y <- fit_data$Salary

x.train <- as.matrix(training[,-1])
y.train <- as.matrix(training$Salary)
x.test <- as.matrix(testing[,-1])
y.test <- as.matrix(testing$Salary)
```

## Ridge / Lasso

```{r}
# ridge
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

ridge.mod = cv.glmnet(x.train, y.train, alpha=0,lambda=lambda.list.ridge, nfolds=5)

ridge.pred_1=predict(ridge.mod, s = ridge.mod$lambda.min, type="coefficients", newx=x.test)

ridge.pred=predict(ridge.mod, s = ridge.mod$lambda.min, newx=x.test)
mean((ridge.pred-y.test)^2)


# cross-validation to choose best tuning parameter
set.seed(3142022)
cv.out.ridge = cv.glmnet(x.train, y.train, alpha= 0)
plot(cv.out.ridge)
abline(v=log(cv.out.ridge$lambda.min), col = "blue", lwd=3, lty=2)

bestlam = cv.out.ridge$lambda.min
bestlam

ridge.pred=predict(ridge.mod, s = bestlam, newx=x.test)
mean((ridge.pred-y.test)^2)

out = glmnet(x,y,alpha=0)
predict(out, type="coefficients", s=bestlam)
```

```{r}
# lasso
set.seed(3142022)
lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length = 100))

lasso.mod <- glmnet(x.train, y.train, alpha=1, lambda=lambda.list.lasso, nfolds = 5)

plot(lasso.mod, xvar="lambda", label=TRUE)

cv.out.lasso = cv.glmnet(x.train,y.train,alpha=1)
plot(cv.out.lasso)
abline(v=log(cv.out.lasso$lambda.min), col="red", lwd=3, lty=2)

bestlam2 = cv.out.lasso$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam2, newx = x.test)
mean((lasso.pred-y.test)^2)

out = glmnet(x, y, alpha=1, lambda=lambda.list.lasso)
lasso.coef = predict(out, type="coefficients", s=bestlam)
lasso.coef

```

## Categorical Decision Tree:

```{r}
# create a new column with High as a binary variable. It is "Yes" if the salary is greater than the median salary
our_data <- our_data %>% mutate(High=as.factor(ifelse(Salary <= median(Salary), "No", "Yes")))
high.test = our_data$High

tree_our_data=tree(Salary~. -Player-High , data = our_data) 
summary(tree_our_data)

# plot the fitted tree
plot(tree_our_data)
text(tree_our_data, pretty = 0, cex = .4, col = "blue")
title("Decision tree on our_data", cex = 0.8)

draw.tree(tree_our_data, nodeinfo=TRUE, cex = 0.4)
```

```{r}
# Fit model on training set
tree.nba = tree(Salary~. , data = training)
 
# Plot the tree
draw.tree(tree.nba, nodeinfo=TRUE, cex = 0.4)
title("Classification Tree Built on Training Set")
 
# Predict on test set
tree.pred = predict(tree.nba, testing, type="vector")
tree.pred

# mean squared error
y = mean((tree.pred-testing$Salary)^2)

# root mean squared error
z=sqrt(y)
z
```

```{r}
set.seed(3142022)

# K=10-Fold cross validation
# cv = cv.tree(tree.nba, FUN= prune.misclass, K=10)
# 
# # the tree with smaller size
# best.cv = min(cv$size[cv$dev == min(cv$dev)])
# best.cv
# 
# # Plot size vs. cross-validation error rate
# plot(cv$size , cv$dev, type="b", xlab = "Number of leaves, \'best\'",
#      ylab = "CV Misclassification Error", col = "red", main="CV")
# abline(v=best.cv, lty=2)
# 
# # Add lines to identify complexity parameter
# min.error = which.min(cv$dev) # Get minimum error index
# abline(h = cv$dev[min.error],lty = 2)
```

```{r}
# Prune tree
# pt.cv = prune.tree(tree.nba, best=best.cv)
# 
# # Plot pruned tree
# plot(pt.cv)
# text(pt.cv, pretty=0, col = "blue", cex = .5)
# title("Pruned tree of size")
```

```{r}
# Calculate the respective test error rate for the model

# Predict on test set
# pred.pt.cv = predict(pt.cv, testing, type="class")
# 
# # examine misclassification errors on training set
# print('training errors')
# classes_test <- as.data.frame(testing) %>% pull(High)
# train_errors_topt <- table(class = classes_test, pred = pred.pt.cv)
# train_errors_topt/rowSums(train_errors_topt)

# test error rate for pt.cv for pt.cv is 0.27907

# The MSE is lower with 7 nodes (8047525). So best. ???
```

## random forest

## PCA

```{r}
dat <- our_data %>% select(-1,-12)
summary(dat)

# check variance of each variable in our_data
apply(dat, 2, var)

# Principle component Analysis
pr.out = prcomp(dat, scale = TRUE)
# center is the mean and scale is the standard deviation
pr.out$center
pr.out$scale
pr.out$rotation

  
# plot 
biplot(pr.out, scale = 0)

# number of prinicipal components needed
pr.out$sdev
pr.var = pr.out$sdev^2
pr.var
pve = pr.var/sum(pr.var)
pve

# plot 
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = "b")
plot(cumsum(pve), xlab=  "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim=c(0,1), type= "b")

# compute PCA
our = scale(dat, center=TRUE, scale=TRUE)
decomp = svd(our)
names(decomp)
decomp$v
pr.out$rotation
decomp$d^2/sum(decomp$d^2)

pve
# we will consider the first two PCAs because they are the largest 2 values. 
```

## Cluster

```{r}
scar  = scale(dat, center=TRUE, scale=TRUE)
km = kmeans(scar, centers=2)
km
fviz_cluster(km, data= dat, gemo= "point", stand = FALSE, frame.type = "norm")

# Dim 1 is more influential. cluster 2 is skewed right

# centering the winner data so we can find out the loadings for the first two PCs

# data_svd <- svd(scar)
# data_loadings <- data_svd$v[,1:2]
# data_pc <- as.matrix(scar) %*% data_loadings 
# colnames(data_loadings) <- colnames(data_pc) <- paste('PC', 1:2, sep = '')
# 
# Player <- our_data$Player %>% factor()
# 
# data_pc %>%
#   as.data.frame() %>%
#   #bind_cols((dplyr:: select(data, state, county, candidate))) %>%
#   ggplot(aes(x = PC1, y = PC2)) +
#   geom_point(alpha = 0.5, aes(color = Player)) +
#   theme_bw()
```

# Conclusion
